{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOzKaL2bWYN5RNNLbdd2DXJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Belldenchoi/Text_Classification_Transformer/blob/main/Text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNgJn8P_3ZX"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip archive.zip -d data/"
      ],
      "metadata": {
        "id": "APFV2FSkH-CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from plotly.offline import plot\n",
        "\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Ubc9iriGIBD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Union, List\n",
        "\n",
        "def preprocess_text(text):\n",
        "    replace_chars = list(string.punctuation + string.digits)\n",
        "    for char in replace_chars:\n",
        "      text = text.replace(char, \" \")\n",
        "\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "\n",
        "    text = emoj.sub(r\" \", text)\n",
        "\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    text = text.lower()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Q6gr3rxLYAt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext==0.17.2\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eylryAS9UQzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "ani-XPV39pfw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "hqd0bk4KqEBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset('thainq107/ntc-scv')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OrqGM9Thpc2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "J7qInIabfvTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(sentences, tokenizer):\n",
        "  for sentence in sentences:\n",
        "    yield tokenizer(sentence)"
      ],
      "metadata": {
        "id": "EzXZcybbuMUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "vocabulary = build_vocab_from_iterator(\n",
        "    yield_tokens(ds['train']['preprocessed_sentence'], tokenizer),\n",
        "    max_tokens = vocab_size,\n",
        "    specials= [\"<pad>\", \"<unk>\"]\n",
        ")"
      ],
      "metadata": {
        "id": "HAZn3PCTfneb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.set_default_index(vocabulary[\"<unk>\"])"
      ],
      "metadata": {
        "id": "E_PIjt5tvu-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.functional import to_map_style_dataset"
      ],
      "metadata": {
        "id": "W1XPTOcIwAPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RchVi74nqkxl",
        "outputId": "c5a7a348-d663-4a41-ea12-d0e5e39a2dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'preprocessed_sentence'],\n",
              "        num_rows: 30000\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['sentence', 'label', 'preprocessed_sentence'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'preprocessed_sentence'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(df):\n",
        "  for row in df:\n",
        "    sentence = row['preprocessed_sentence']\n",
        "    encoded_sentence = vocabulary(tokenizer(sentence))\n",
        "    label = row['label']\n",
        "    yield encoded_sentence, label\n",
        "\n",
        "train_dataset = prepare_dataset(ds['train'])\n",
        "train_dataset = to_map_style_dataset(train_dataset)\n",
        "\n",
        "valid_dataset = prepare_dataset(ds['valid'])\n",
        "valid_dataset = to_map_style_dataset(valid_dataset)\n",
        "\n",
        "test_dataset = prepare_dataset(ds['test'])\n",
        "test_dataset = to_map_style_dataset(test_dataset)"
      ],
      "metadata": {
        "id": "NLeZY8kjw3ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "CB06D7Wfzc1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "yL1OckpT57Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "\n",
        "def collate_batch(batch):\n",
        "  sentences, labels = list(zip(*batch))\n",
        "\n",
        "  encoded_sentences = [sentence + ([0] * (seq_length - len(sentence))) if len(sentence) < seq_length else sentence[:seq_length] for sentence in sentences]\n",
        "\n",
        "  encoded_sentences = torch.tensor(encoded_sentences, dtype=torch.int64)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  return encoded_sentences, labels\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "iyj9Iu4TyzRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, max_length, device = 'cpu'):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.word_emb = nn.Embedding(\n",
        "        num_embeddings= vocab_size,\n",
        "        embedding_dim= embed_dim\n",
        "    )\n",
        "\n",
        "    self.pos_emb = nn.Embedding(\n",
        "        num_embeddings=max_length,\n",
        "        embedding_dim= embed_dim\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, seq_len = x.size()\n",
        "    positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
        "    output1 = self.word_emb(x)\n",
        "    output2 = self.pos_emb(positions)\n",
        "    output = output1 + output2\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "cXjoefFW1U5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.attn = nn.MultiheadAttention(\n",
        "        embed_dim = embed_dim,\n",
        "        num_heads = num_heads,\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "    )\n",
        "    self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps = 1e-6)\n",
        "    self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps = 1e-6)\n",
        "    self.dropout_1 = nn.Dropout(p = dropout)\n",
        "    self.dropout_2 = nn.Dropout(p = dropout)\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    attn_output, _ = self.attn(query, key, value)\n",
        "    attn_output = self.dropout_1(attn_output)\n",
        "    out_1 = self.layernorm_1(query + attn_output)\n",
        "    ffn_output = self.ffn(out_1)\n",
        "    ffn_output = self.dropout_2(ffn_output)\n",
        "    out_2 = self.layernorm_2(out_1 + ffn_output)\n",
        "    return out_2\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, src_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout = 0.1, device = 'cpu'):\n",
        "    super().__init__()\n",
        "    self.embedding = TokenAndPositionEmbedding(\n",
        "        src_vocab_size, embed_dim, max_length, device\n",
        "    )\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerEncoderBlock(\n",
        "                embed_dim, num_heads, ff_dim, dropout\n",
        "            ) for i in range(num_layers)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.embedding(x)\n",
        "    for layer in self.layers:\n",
        "        output = layer(output, output, output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "7oroZ17c6syD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderCls(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, num_layers, embed_dim, num_heads, ff_dim, dropout = 0.1, device = 'cpu'):\n",
        "    super().__init__()\n",
        "    self.encoder = TransformerEncoder(\n",
        "        vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout, device\n",
        "    )\n",
        "    self.pooling = nn.AvgPool1d(kernel_size=max_length)\n",
        "    self.fc1 = nn.Linear(in_features=embed_dim, out_features=20)\n",
        "    self.fc2 = nn.Linear(in_features=20, out_features=2)\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.encoder(x)\n",
        "    output = self.pooling(output.permute(0,2,1)).squeeze()\n",
        "    output = self.dropout(output)\n",
        "    output = self.fc1(output)\n",
        "    output = self.dropout(output)\n",
        "    output = self.fc2(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "EdFaTsZz0qfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_epoch(model, optimizer, criterion, train_dataloader, device, epoch = 0, log_interval = 50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      predictions = model(inputs)\n",
        "\n",
        "      loss = criterion(predictions, labels)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "      total_count += labels.size(0)\n",
        "\n",
        "      if idx % log_interval == 0 and idx > 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\n",
        "            \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "            \"| accuracy {:8.3f}\".format(\n",
        "                epoch, idx, len(train_dataloader), total_acc / total_count\n",
        "            )\n",
        "        )\n",
        "        total_acc, total_count = 0, 0\n",
        "        start_time = time.time()\n",
        "\n",
        "    epoch_acc = total_acc / total_count\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "    return epoch_acc, epoch_loss\n",
        "\n",
        "\n",
        "def evaluate_epoch(model, criterion , valid_dataloader, device):\n",
        "  model.eval()\n",
        "  total_acc, total_count = 0,0\n",
        "  losses = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (inputs, labels) in enumerate(valid_dataloader):\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      predictions = model(inputs)\n",
        "\n",
        "      loss = criterion(predictions, labels)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "      total_count += labels.size(0)\n",
        "\n",
        "  epoch_acc = total_acc/total_count\n",
        "  epoch_loss = sum(losses)/len(losses)\n",
        "  return epoch_acc, epoch_loss\n",
        "\n",
        "def train(model, model_name, save_model, optimizer, criterion, train_dataloader, valid_dataloader, num_epochs, device):\n",
        "  train_accs , train_losses = [], []\n",
        "  eval_accs , eval_losses = [], []\n",
        "  best_loss_eval = 100\n",
        "  times = []\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_acc, train_loss = train_epoch(model, optimizer, criterion, train_dataloader, device, epoch)\n",
        "    train_accs.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    eval_acc, eval_loss = evaluate_epoch(model, criterion, valid_dataloader, device)\n",
        "    eval_accs.append(eval_acc)\n",
        "    eval_losses.append(eval_loss)\n",
        "\n",
        "    if eval_loss < best_loss_eval:\n",
        "      torch.save(model.state_dict(), save_model + f'/{model_name}.pt')\n",
        "\n",
        "    times.append(time.time() - epoch_start_time)\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "    print(\n",
        "        \"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} \"\n",
        "        \"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f} \".format(\n",
        "            epoch, time.time() - epoch_start_time, train_acc, train_loss, eval_acc, eval_loss\n",
        "        )\n",
        "    )\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "  model.load_state_dict(torch.load(save_model + f'/{model_name}.pt'))\n",
        "  model.eval()\n",
        "\n",
        "  metrics = {\n",
        "      'train_accuracy': train_accs,\n",
        "      'train_loss': train_losses,\n",
        "      'valid_accuracy': eval_accs,\n",
        "      'valid_loss': eval_losses,\n",
        "      'time': times\n",
        "  }\n",
        "\n",
        "  return model, metrics\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses):\n",
        "  epochs = list(range(num_epochs))\n",
        "  fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (12,6))\n",
        "\n",
        "  axs[0].plot(epochs, train_accs, label = \"Training\")\n",
        "  axs[0].plot(epochs, eval_accs, label = \"Evaluation\")\n",
        "  axs[1].plot(epochs, train_losses, label = \"Training\")\n",
        "  axs[1].plot(epochs, eval_losses, label = \"Evaluation\")\n",
        "  axs[0].set_xlabel(\"Epochs\")\n",
        "  axs[1].set_xlabel(\"Epochs\")\n",
        "  axs[0].set_ylabel(\"Accuracy\")\n",
        "  axs[1].set_ylabel(\"Loss\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "wAUEctQm_0iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "embed_dim = 200\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "dropout = 0.1\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = TransformerEncoderCls(\n",
        "    vocab_size, max_length, num_layers, embed_dim, num_heads, ff_dim, dropout, device)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 30\n",
        "save_model = './model'\n",
        "os.makedirs(save_model, exist_ok = True)\n",
        "model_name = \"model\"\n",
        "\n",
        "model, metrics = train(\n",
        "    model, model_name, save_model, optimizer, criterion, train_dataloader, valid_dataloader, num_epochs, device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiMuBz_nJHri",
        "outputId": "3c9e3ff1-056b-49f4-e4d0-a6e4401d0c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  235 batches | accuracy    0.550\n",
            "| epoch   1 |   100/  235 batches | accuracy    0.596\n",
            "| epoch   1 |   150/  235 batches | accuracy    0.666\n",
            "| epoch   1 |   200/  235 batches | accuracy    0.715\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   1 | Time:  4.30s | Train Accuracy    0.723 | Train Loss    0.656 | Valid Accuracy    0.752 | Valid Loss    0.569 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |    50/  235 batches | accuracy    0.756\n",
            "| epoch   2 |   100/  235 batches | accuracy    0.790\n",
            "| epoch   2 |   150/  235 batches | accuracy    0.798\n",
            "| epoch   2 |   200/  235 batches | accuracy    0.802\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   2 | Time:  4.31s | Train Accuracy    0.811 | Train Loss    0.515 | Valid Accuracy    0.804 | Valid Loss    0.501 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |    50/  235 batches | accuracy    0.809\n",
            "| epoch   3 |   100/  235 batches | accuracy    0.810\n",
            "| epoch   3 |   150/  235 batches | accuracy    0.827\n",
            "| epoch   3 |   200/  235 batches | accuracy    0.822\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   3 | Time:  4.32s | Train Accuracy    0.820 | Train Loss    0.477 | Valid Accuracy    0.803 | Valid Loss    0.495 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |    50/  235 batches | accuracy    0.829\n",
            "| epoch   4 |   100/  235 batches | accuracy    0.828\n",
            "| epoch   4 |   150/  235 batches | accuracy    0.841\n",
            "| epoch   4 |   200/  235 batches | accuracy    0.833\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   4 | Time:  4.34s | Train Accuracy    0.825 | Train Loss    0.461 | Valid Accuracy    0.821 | Valid Loss    0.474 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |    50/  235 batches | accuracy    0.838\n",
            "| epoch   5 |   100/  235 batches | accuracy    0.844\n",
            "| epoch   5 |   150/  235 batches | accuracy    0.838\n",
            "| epoch   5 |   200/  235 batches | accuracy    0.837\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   5 | Time:  4.34s | Train Accuracy    0.839 | Train Loss    0.451 | Valid Accuracy    0.828 | Valid Loss    0.466 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |    50/  235 batches | accuracy    0.840\n",
            "| epoch   6 |   100/  235 batches | accuracy    0.844\n",
            "| epoch   6 |   150/  235 batches | accuracy    0.843\n",
            "| epoch   6 |   200/  235 batches | accuracy    0.846\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   6 | Time:  4.36s | Train Accuracy    0.850 | Train Loss    0.443 | Valid Accuracy    0.830 | Valid Loss    0.462 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |    50/  235 batches | accuracy    0.847\n",
            "| epoch   7 |   100/  235 batches | accuracy    0.841\n",
            "| epoch   7 |   150/  235 batches | accuracy    0.861\n",
            "| epoch   7 |   200/  235 batches | accuracy    0.850\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   7 | Time:  4.37s | Train Accuracy    0.855 | Train Loss    0.436 | Valid Accuracy    0.832 | Valid Loss    0.461 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |    50/  235 batches | accuracy    0.859\n",
            "| epoch   8 |   100/  235 batches | accuracy    0.853\n",
            "| epoch   8 |   150/  235 batches | accuracy    0.852\n",
            "| epoch   8 |   200/  235 batches | accuracy    0.860\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   8 | Time:  4.37s | Train Accuracy    0.852 | Train Loss    0.429 | Valid Accuracy    0.836 | Valid Loss    0.455 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |    50/  235 batches | accuracy    0.863\n",
            "| epoch   9 |   100/  235 batches | accuracy    0.864\n",
            "| epoch   9 |   150/  235 batches | accuracy    0.858\n",
            "| epoch   9 |   200/  235 batches | accuracy    0.857\n",
            "-----------------------------------------------------------\n",
            "| End of epoch   9 | Time:  4.36s | Train Accuracy    0.867 | Train Loss    0.422 | Valid Accuracy    0.836 | Valid Loss    0.456 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |    50/  235 batches | accuracy    0.869\n",
            "| epoch  10 |   100/  235 batches | accuracy    0.864\n",
            "| epoch  10 |   150/  235 batches | accuracy    0.861\n",
            "| epoch  10 |   200/  235 batches | accuracy    0.865\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  10 | Time:  4.36s | Train Accuracy    0.860 | Train Loss    0.416 | Valid Accuracy    0.835 | Valid Loss    0.456 \n",
            "-----------------------------------------------------------\n",
            "| epoch  11 |    50/  235 batches | accuracy    0.870\n",
            "| epoch  11 |   100/  235 batches | accuracy    0.872\n",
            "| epoch  11 |   150/  235 batches | accuracy    0.864\n",
            "| epoch  11 |   200/  235 batches | accuracy    0.874\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  11 | Time:  4.35s | Train Accuracy    0.869 | Train Loss    0.410 | Valid Accuracy    0.841 | Valid Loss    0.448 \n",
            "-----------------------------------------------------------\n",
            "| epoch  12 |    50/  235 batches | accuracy    0.875\n",
            "| epoch  12 |   100/  235 batches | accuracy    0.879\n",
            "| epoch  12 |   150/  235 batches | accuracy    0.872\n",
            "| epoch  12 |   200/  235 batches | accuracy    0.872\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  12 | Time:  4.34s | Train Accuracy    0.866 | Train Loss    0.404 | Valid Accuracy    0.842 | Valid Loss    0.447 \n",
            "-----------------------------------------------------------\n",
            "| epoch  13 |    50/  235 batches | accuracy    0.879\n",
            "| epoch  13 |   100/  235 batches | accuracy    0.873\n",
            "| epoch  13 |   150/  235 batches | accuracy    0.878\n",
            "| epoch  13 |   200/  235 batches | accuracy    0.880\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  13 | Time:  4.33s | Train Accuracy    0.874 | Train Loss    0.398 | Valid Accuracy    0.842 | Valid Loss    0.450 \n",
            "-----------------------------------------------------------\n",
            "| epoch  14 |    50/  235 batches | accuracy    0.882\n",
            "| epoch  14 |   100/  235 batches | accuracy    0.886\n",
            "| epoch  14 |   150/  235 batches | accuracy    0.886\n",
            "| epoch  14 |   200/  235 batches | accuracy    0.885\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  14 | Time:  4.34s | Train Accuracy    0.872 | Train Loss    0.393 | Valid Accuracy    0.844 | Valid Loss    0.450 \n",
            "-----------------------------------------------------------\n",
            "| epoch  15 |    50/  235 batches | accuracy    0.887\n",
            "| epoch  15 |   100/  235 batches | accuracy    0.891\n",
            "| epoch  15 |   150/  235 batches | accuracy    0.882\n",
            "| epoch  15 |   200/  235 batches | accuracy    0.883\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  15 | Time:  4.32s | Train Accuracy    0.879 | Train Loss    0.387 | Valid Accuracy    0.839 | Valid Loss    0.454 \n",
            "-----------------------------------------------------------\n",
            "| epoch  16 |    50/  235 batches | accuracy    0.886\n",
            "| epoch  16 |   100/  235 batches | accuracy    0.887\n",
            "| epoch  16 |   150/  235 batches | accuracy    0.892\n",
            "| epoch  16 |   200/  235 batches | accuracy    0.891\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  16 | Time:  4.32s | Train Accuracy    0.892 | Train Loss    0.381 | Valid Accuracy    0.843 | Valid Loss    0.452 \n",
            "-----------------------------------------------------------\n",
            "| epoch  17 |    50/  235 batches | accuracy    0.891\n",
            "| epoch  17 |   100/  235 batches | accuracy    0.893\n",
            "| epoch  17 |   150/  235 batches | accuracy    0.892\n",
            "| epoch  17 |   200/  235 batches | accuracy    0.897\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  17 | Time:  4.33s | Train Accuracy    0.883 | Train Loss    0.377 | Valid Accuracy    0.845 | Valid Loss    0.448 \n",
            "-----------------------------------------------------------\n",
            "| epoch  18 |    50/  235 batches | accuracy    0.906\n",
            "| epoch  18 |   100/  235 batches | accuracy    0.896\n",
            "| epoch  18 |   150/  235 batches | accuracy    0.891\n",
            "| epoch  18 |   200/  235 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  18 | Time:  4.32s | Train Accuracy    0.893 | Train Loss    0.372 | Valid Accuracy    0.843 | Valid Loss    0.452 \n",
            "-----------------------------------------------------------\n",
            "| epoch  19 |    50/  235 batches | accuracy    0.907\n",
            "| epoch  19 |   100/  235 batches | accuracy    0.899\n",
            "| epoch  19 |   150/  235 batches | accuracy    0.902\n",
            "| epoch  19 |   200/  235 batches | accuracy    0.895\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  19 | Time:  4.31s | Train Accuracy    0.906 | Train Loss    0.365 | Valid Accuracy    0.844 | Valid Loss    0.456 \n",
            "-----------------------------------------------------------\n",
            "| epoch  20 |    50/  235 batches | accuracy    0.910\n",
            "| epoch  20 |   100/  235 batches | accuracy    0.901\n",
            "| epoch  20 |   150/  235 batches | accuracy    0.901\n",
            "| epoch  20 |   200/  235 batches | accuracy    0.902\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  20 | Time:  4.32s | Train Accuracy    0.917 | Train Loss    0.359 | Valid Accuracy    0.836 | Valid Loss    0.482 \n",
            "-----------------------------------------------------------\n",
            "| epoch  21 |    50/  235 batches | accuracy    0.912\n",
            "| epoch  21 |   100/  235 batches | accuracy    0.899\n",
            "| epoch  21 |   150/  235 batches | accuracy    0.915\n",
            "| epoch  21 |   200/  235 batches | accuracy    0.913\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  21 | Time:  4.32s | Train Accuracy    0.910 | Train Loss    0.355 | Valid Accuracy    0.842 | Valid Loss    0.465 \n",
            "-----------------------------------------------------------\n",
            "| epoch  22 |    50/  235 batches | accuracy    0.912\n",
            "| epoch  22 |   100/  235 batches | accuracy    0.916\n",
            "| epoch  22 |   150/  235 batches | accuracy    0.919\n",
            "| epoch  22 |   200/  235 batches | accuracy    0.911\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  22 | Time:  4.31s | Train Accuracy    0.919 | Train Loss    0.347 | Valid Accuracy    0.840 | Valid Loss    0.472 \n",
            "-----------------------------------------------------------\n",
            "| epoch  23 |    50/  235 batches | accuracy    0.922\n",
            "| epoch  23 |   100/  235 batches | accuracy    0.924\n",
            "| epoch  23 |   150/  235 batches | accuracy    0.922\n",
            "| epoch  23 |   200/  235 batches | accuracy    0.920\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  23 | Time:  4.33s | Train Accuracy    0.912 | Train Loss    0.341 | Valid Accuracy    0.841 | Valid Loss    0.467 \n",
            "-----------------------------------------------------------\n",
            "| epoch  24 |    50/  235 batches | accuracy    0.928\n",
            "| epoch  24 |   100/  235 batches | accuracy    0.934\n",
            "| epoch  24 |   150/  235 batches | accuracy    0.924\n",
            "| epoch  24 |   200/  235 batches | accuracy    0.920\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  24 | Time:  4.32s | Train Accuracy    0.925 | Train Loss    0.334 | Valid Accuracy    0.840 | Valid Loss    0.483 \n",
            "-----------------------------------------------------------\n",
            "| epoch  25 |    50/  235 batches | accuracy    0.934\n",
            "| epoch  25 |   100/  235 batches | accuracy    0.928\n",
            "| epoch  25 |   150/  235 batches | accuracy    0.931\n",
            "| epoch  25 |   200/  235 batches | accuracy    0.925\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  25 | Time:  4.33s | Train Accuracy    0.928 | Train Loss    0.329 | Valid Accuracy    0.838 | Valid Loss    0.484 \n",
            "-----------------------------------------------------------\n",
            "| epoch  26 |    50/  235 batches | accuracy    0.937\n",
            "| epoch  26 |   100/  235 batches | accuracy    0.935\n",
            "| epoch  26 |   150/  235 batches | accuracy    0.934\n",
            "| epoch  26 |   200/  235 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  26 | Time:  4.34s | Train Accuracy    0.931 | Train Loss    0.322 | Valid Accuracy    0.838 | Valid Loss    0.487 \n",
            "-----------------------------------------------------------\n",
            "| epoch  27 |    50/  235 batches | accuracy    0.946\n",
            "| epoch  27 |   100/  235 batches | accuracy    0.937\n",
            "| epoch  27 |   150/  235 batches | accuracy    0.938\n",
            "| epoch  27 |   200/  235 batches | accuracy    0.940\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  27 | Time:  4.33s | Train Accuracy    0.940 | Train Loss    0.314 | Valid Accuracy    0.838 | Valid Loss    0.506 \n",
            "-----------------------------------------------------------\n",
            "| epoch  28 |    50/  235 batches | accuracy    0.943\n",
            "| epoch  28 |   100/  235 batches | accuracy    0.946\n",
            "| epoch  28 |   150/  235 batches | accuracy    0.945\n",
            "| epoch  28 |   200/  235 batches | accuracy    0.949\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  28 | Time:  4.34s | Train Accuracy    0.941 | Train Loss    0.306 | Valid Accuracy    0.837 | Valid Loss    0.497 \n",
            "-----------------------------------------------------------\n",
            "| epoch  29 |    50/  235 batches | accuracy    0.951\n",
            "| epoch  29 |   100/  235 batches | accuracy    0.951\n",
            "| epoch  29 |   150/  235 batches | accuracy    0.950\n",
            "| epoch  29 |   200/  235 batches | accuracy    0.948\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  29 | Time:  4.34s | Train Accuracy    0.951 | Train Loss    0.300 | Valid Accuracy    0.835 | Valid Loss    0.508 \n",
            "-----------------------------------------------------------\n",
            "| epoch  30 |    50/  235 batches | accuracy    0.959\n",
            "| epoch  30 |   100/  235 batches | accuracy    0.954\n",
            "| epoch  30 |   150/  235 batches | accuracy    0.952\n",
            "| epoch  30 |   200/  235 batches | accuracy    0.947\n",
            "-----------------------------------------------------------\n",
            "| End of epoch  30 | Time:  4.34s | Train Accuracy    0.949 | Train Loss    0.296 | Valid Accuracy    0.833 | Valid Loss    0.522 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_result(num_epochs, metrics['train_accuracy'], metrics['valid_accuracy'], metrics['train_loss'], metrics['valid_loss'])"
      ],
      "metadata": {
        "id": "02IMIBq5c10A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mauzbjp6cS6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}